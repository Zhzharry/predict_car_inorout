"""
æŒ‡æ ‡è®¡ç®—å’ŒæŠ¥å‘Šè¾“å‡ºæ¨¡å—
åŠŸèƒ½ï¼šè®¡ç®—CSVæ–‡ä»¶ä¸­æ ‡ç­¾åˆ—å’Œresultåˆ—çš„å„ç±»è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è¾“å‡ºè¯¦ç»†çš„txtæŠ¥å‘Š
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, cohen_kappa_score,
    matthews_corrcoef, hamming_loss, jaccard_score
)
import os
import time
from datetime import datetime
from c_æ¨¡å‹è°ƒç”¨ä»¥åŠç»“æœåˆ†æä»£ç  import ModelPredictor


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—ç±»"""
    
    def __init__(self, csv_path, label_column='labelArea', result_column='result', 
                 output_dir='results/'):
        """
        åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        
        Parameters:
        -----------
        csv_path : str
            CSVæ–‡ä»¶è·¯å¾„
        label_column : str
            çœŸå®æ ‡ç­¾åˆ—å
        result_column : str
            é¢„æµ‹ç»“æœåˆ—å
        output_dir : str
            æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.csv_path = csv_path
        self.label_column = label_column
        self.result_column = result_column
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # æ•°æ®
        self.df = None
        self.y_true = None
        self.y_pred = None
        
        # æŒ‡æ ‡ç»“æœ
        self.metrics = {}
        
    def load_data(self):
        """åŠ è½½æ•°æ®å¹¶æå–æ ‡ç­¾å’Œé¢„æµ‹ç»“æœ"""
        print("=" * 60)
        print("åŠ è½½æ•°æ®")
        print("=" * 60)
        
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {self.csv_path}")
        
        self.df = pd.read_csv(self.csv_path)
        print(f"æ•°æ®å½¢çŠ¶: {self.df.shape}")
        print(f"åˆ—å: {list(self.df.columns)}")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        if self.label_column not in self.df.columns:
            raise ValueError(f"æ ‡ç­¾åˆ— '{self.label_column}' ä¸å­˜åœ¨")
        if self.result_column not in self.df.columns:
            raise ValueError(f"ç»“æœåˆ— '{self.result_column}' ä¸å­˜åœ¨")
        
        # æå–æ ‡ç­¾å’Œé¢„æµ‹ç»“æœ
        self.y_true = self.df[self.label_column].copy()
        self.y_pred = self.df[self.result_column].copy()
        
        # å¤„ç†ç¼ºå¤±å€¼å’Œç©ºå€¼ï¼ˆåŒ…æ‹¬NaNã€ç©ºå­—ç¬¦ä¸²ã€ç©ºæ ¼ç­‰ï¼‰
        # å°†ç©ºå­—ç¬¦ä¸²ã€ç©ºæ ¼ç­‰è½¬æ¢ä¸ºNaN
        if self.y_true.dtype == 'object':
            self.y_true = self.y_true.replace(['', ' ', 'nan', 'NaN', 'None', np.nan], np.nan)
        else:
            # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œåªå¤„ç†NaN
            self.y_true = self.y_true.replace([np.nan], np.nan)
        
        if self.y_pred.dtype == 'object':
            self.y_pred = self.y_pred.replace(['', ' ', 'nan', 'NaN', 'None', np.nan], np.nan)
        else:
            # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œåªå¤„ç†NaN
            self.y_pred = self.y_pred.replace([np.nan], np.nan)
        
        # è¿‡æ»¤ç¼ºå¤±å€¼
        valid_mask = ~(self.y_true.isna() | self.y_pred.isna())
        self.y_true = self.y_true[valid_mask]
        self.y_pred = self.y_pred[valid_mask]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if len(self.y_true) == 0:
            # ç»Ÿè®¡åŸå§‹æ•°æ®æƒ…å†µ
            original_true = self.df[self.label_column]
            original_pred = self.df[self.result_column]
            
            true_nan = original_true.isna().sum()
            pred_nan = original_pred.isna().sum()
            
            # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯objectç±»å‹ï¼‰
            if original_true.dtype == 'object':
                true_empty = (original_true == '').sum()
            else:
                true_empty = 0
            
            if original_pred.dtype == 'object':
                pred_empty = (original_pred == '').sum()
            else:
                pred_empty = 0
            
            # åˆ¤æ–­ä¸»è¦é—®é¢˜
            if pred_nan == len(self.df):
                problem_msg = f"âŒ é—®é¢˜ï¼š'{self.result_column}' åˆ—å®Œå…¨ä¸ºç©ºï¼ˆæ‰€æœ‰ {len(self.df)} ä¸ªå€¼éƒ½æ˜¯ NaNï¼‰\n"
                solution_msg = (
                    f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"   1. å¦‚æœè¿™æ˜¯æµ‹è¯•æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ¨¡å‹é¢„æµ‹ä»£ç æ¥å¡«å……é¢„æµ‹ç»“æœï¼š\n"
                    f"      python common/c_æ¨¡å‹è°ƒç”¨ä»¥åŠç»“æœåˆ†æä»£ç .py\n"
                    f"   2. å¦‚æœè¿™æ˜¯è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ¨¡å‹è®­ç»ƒä»£ç ï¼š\n"
                    f"      python common/b_æ¨¡å‹è®­ç»ƒä»£ç .py\n"
                    f"      ï¼ˆè®­ç»ƒä»£ç ä¼šè‡ªåŠ¨å¡«å……è®­ç»ƒæ•°æ®çš„resultåˆ—ï¼‰\n"
                )
            elif true_nan == len(self.df):
                problem_msg = f"âŒ é—®é¢˜ï¼š'{self.label_column}' åˆ—å®Œå…¨ä¸ºç©ºï¼ˆæ‰€æœ‰ {len(self.df)} ä¸ªå€¼éƒ½æ˜¯ NaNï¼‰\n"
                solution_msg = (
                    f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"   1. å¦‚æœè¿™æ˜¯æµ‹è¯•æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®å›å¡«ä»£ç æ¥å¡«å……çœŸå®æ ‡ç­¾ï¼š\n"
                    f"      python common/d_æ•°æ®å›å¡«ä»£ç .py\n"
                    f"   2. æˆ–è€…æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼Œç¡®ä¿labelAreaåˆ—æœ‰æœ‰æ•ˆå€¼\n"
                )
            else:
                problem_msg = f"âŒ é—®é¢˜ï¼š'{self.label_column}' å’Œ '{self.result_column}' åˆ—éƒ½æœ‰ç¼ºå¤±å€¼\n"
                solution_msg = (
                    f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"   è¯·ç¡®ä¿æ ‡ç­¾åˆ—å’Œç»“æœåˆ—éƒ½æœ‰æœ‰æ•ˆå€¼\n"
                )
            
            raise ValueError(
                f"{'='*60}\n"
                f"é”™è¯¯ï¼šè¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼\n"
                f"{'='*60}\n"
                f"{problem_msg}\n"
                f"æ•°æ®ç»Ÿè®¡ï¼š\n"
                f"  - åŸå§‹æ•°æ®æ€»è¡Œæ•°: {len(self.df)}\n"
                f"  - æ ‡ç­¾åˆ— '{self.label_column}':\n"
                f"    * ç¼ºå¤±å€¼(NaN): {true_nan} ä¸ª ({true_nan/len(self.df)*100:.1f}%)\n"
                f"    * ç©ºå­—ç¬¦ä¸²: {true_empty} ä¸ª\n"
                f"    * æ•°æ®ç±»å‹: {original_true.dtype}\n"
                f"  - ç»“æœåˆ— '{self.result_column}':\n"
                f"    * ç¼ºå¤±å€¼(NaN): {pred_nan} ä¸ª ({pred_nan/len(self.df)*100:.1f}%)\n"
                f"    * ç©ºå­—ç¬¦ä¸²: {pred_empty} ä¸ª\n"
                f"    * æ•°æ®ç±»å‹: {original_pred.dtype}\n"
                f"{solution_msg}\n"
                f"{'='*60}"
            )
        
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(self.y_true)}")
        print(f"çœŸå®æ ‡ç­¾åˆ†å¸ƒ:\n{self.y_true.value_counts().sort_index()}")
        print(f"é¢„æµ‹ç»“æœåˆ†å¸ƒ:\n{self.y_pred.value_counts().sort_index()}")
        
        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼ˆå¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸²ç±»å‹ï¼‰
        try:
            self.y_true = pd.to_numeric(self.y_true, errors='coerce').astype(int)
            self.y_pred = pd.to_numeric(self.y_pred, errors='coerce').astype(int)
        except (ValueError, TypeError) as e:
            raise ValueError(
                f"æ— æ³•å°†æ ‡ç­¾æˆ–ç»“æœè½¬æ¢ä¸ºæ•´æ•°ç±»å‹ã€‚\n"
                f"æ ‡ç­¾åˆ—å”¯ä¸€å€¼: {self.y_true.unique()[:10]}\n"
                f"ç»“æœåˆ—å”¯ä¸€å€¼: {self.y_pred.unique()[:10]}\n"
                f"é”™è¯¯: {e}"
            )
        
        return self.y_true, self.y_pred
    
    def calculate_all_metrics(self):
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        print("\n" + "=" * 60)
        print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
        print("=" * 60)
        
        if self.y_true is None or self.y_pred is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if len(self.y_true) == 0 or len(self.y_pred) == 0:
            raise ValueError("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
        
        # è·å–ç±»åˆ«æ ‡ç­¾
        classes = sorted(set(self.y_true.unique()) | set(self.y_pred.unique()))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç±»åˆ«
        if len(classes) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç±»åˆ«æ ‡ç­¾ã€‚è¯·æ£€æŸ¥æ•°æ®ã€‚")
        
        class_names = {0: 'å…¶ä»–åœºæ™¯', 1: 'è¿›å…¥åœ°åº“', 2: 'å‡ºåœ°åº“'}
        
        # 1. åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(self.y_true, self.y_pred)
        self.metrics['accuracy'] = accuracy
        
        # 2. ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼ˆæ¯ä¸ªç±»åˆ«å’Œå¹³å‡ï¼‰
        precision_per_class = precision_score(self.y_true, self.y_pred, 
                                            labels=classes, average=None, zero_division=0)
        recall_per_class = recall_score(self.y_true, self.y_pred, 
                                       labels=classes, average=None, zero_division=0)
        f1_per_class = f1_score(self.y_true, self.y_pred, 
                               labels=classes, average=None, zero_division=0)
        
        precision_macro = precision_score(self.y_true, self.y_pred, 
                                        labels=classes, average='macro', zero_division=0)
        recall_macro = recall_score(self.y_true, self.y_pred, 
                                   labels=classes, average='macro', zero_division=0)
        f1_macro = f1_score(self.y_true, self.y_pred, 
                          labels=classes, average='macro', zero_division=0)
        
        precision_weighted = precision_score(self.y_true, self.y_pred, 
                                           labels=classes, average='weighted', zero_division=0)
        recall_weighted = recall_score(self.y_true, self.y_pred, 
                                      labels=classes, average='weighted', zero_division=0)
        f1_weighted = f1_score(self.y_true, self.y_pred, 
                             labels=classes, average='weighted', zero_division=0)
        
        precision_micro = precision_score(self.y_true, self.y_pred, 
                                        labels=classes, average='micro', zero_division=0)
        recall_micro = recall_score(self.y_true, self.y_pred, 
                                   labels=classes, average='micro', zero_division=0)
        f1_micro = f1_score(self.y_true, self.y_pred, 
                          labels=classes, average='micro', zero_division=0)
        
        self.metrics['precision_per_class'] = dict(zip(classes, precision_per_class))
        self.metrics['recall_per_class'] = dict(zip(classes, recall_per_class))
        self.metrics['f1_per_class'] = dict(zip(classes, f1_per_class))
        self.metrics['precision_macro'] = precision_macro
        self.metrics['recall_macro'] = recall_macro
        self.metrics['f1_macro'] = f1_macro
        self.metrics['precision_weighted'] = precision_weighted
        self.metrics['recall_weighted'] = recall_weighted
        self.metrics['f1_weighted'] = f1_weighted
        self.metrics['precision_micro'] = precision_micro
        self.metrics['recall_micro'] = recall_micro
        self.metrics['f1_micro'] = f1_micro
        
        # 3. æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_true, self.y_pred, labels=classes)
        self.metrics['confusion_matrix'] = cm
        self.metrics['classes'] = classes
        
        # 4. æ”¯æŒæ•°ï¼ˆæ¯ä¸ªç±»åˆ«çš„çœŸå®æ ·æœ¬æ•°ï¼‰
        support = {}
        for cls in classes:
            support[cls] = int(np.sum(self.y_true == cls))
        self.metrics['support'] = support
        
        # 5. å…¶ä»–æŒ‡æ ‡
        kappa = cohen_kappa_score(self.y_true, self.y_pred)
        mcc = matthews_corrcoef(self.y_true, self.y_pred)
        hamming = hamming_loss(self.y_true, self.y_pred)
        
        # Jaccardåˆ†æ•°ï¼ˆæ¯ä¸ªç±»åˆ«ï¼‰
        jaccard_per_class = jaccard_score(self.y_true, self.y_pred, 
                                         labels=classes, average=None, zero_division=0)
        jaccard_macro = jaccard_score(self.y_true, self.y_pred, 
                                     labels=classes, average='macro', zero_division=0)
        jaccard_weighted = jaccard_score(self.y_true, self.y_pred, 
                                        labels=classes, average='weighted', zero_division=0)
        
        self.metrics['kappa'] = kappa
        self.metrics['mcc'] = mcc
        self.metrics['hamming_loss'] = hamming
        self.metrics['jaccard_per_class'] = dict(zip(classes, jaccard_per_class))
        self.metrics['jaccard_macro'] = jaccard_macro
        self.metrics['jaccard_weighted'] = jaccard_weighted
        
        # 6. é”™è¯¯åˆ†æ
        error_rate_per_class = {}
        for cls in classes:
            mask = self.y_true == cls
            if np.sum(mask) > 0:
                error_rate = 1 - recall_per_class[classes.index(cls)]
                error_rate_per_class[cls] = error_rate
            else:
                error_rate_per_class[cls] = 0.0
        self.metrics['error_rate_per_class'] = error_rate_per_class
        
        # 7. åˆ†ç±»æŠ¥å‘Šï¼ˆsklearnæ ¼å¼ï¼‰
        self.metrics['classification_report'] = classification_report(
            self.y_true, self.y_pred, 
            labels=classes,
            target_names=[class_names.get(cls, f'ç±»åˆ«{cls}') for cls in classes],
            output_dict=True,
            zero_division=0
        )
        
        print("æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        return self.metrics
    
    def generate_report(self, output_filename=None):
        """ç”Ÿæˆè¯¦ç»†çš„txtæŠ¥å‘Š"""
        if not self.metrics:
            raise ValueError("è¯·å…ˆè®¡ç®—æŒ‡æ ‡")
        
        if output_filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = os.path.basename(self.csv_path).replace('.csv', '')
            output_filename = f"{filename}_æŒ‡æ ‡æŠ¥å‘Š_{timestamp}.txt"
        
        output_path = os.path.join(self.output_dir, output_filename)
        
        print(f"\nç”ŸæˆæŠ¥å‘Š: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            # æŠ¥å‘Šå¤´éƒ¨
            f.write("=" * 80 + "\n")
            f.write("æ¨¡å‹è¯„ä¼°æŒ‡æ ‡æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"æ•°æ®æ–‡ä»¶: {self.csv_path}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {len(self.y_true)}\n")
            f.write(f"æ ‡ç­¾åˆ—: {self.label_column}\n")
            f.write(f"ç»“æœåˆ—: {self.result_column}\n\n")
            
            # 1. æ•°æ®æ¦‚è§ˆ
            f.write("=" * 80 + "\n")
            f.write("1. æ•°æ®æ¦‚è§ˆ\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("çœŸå®æ ‡ç­¾åˆ†å¸ƒ:\n")
            true_dist = self.y_true.value_counts().sort_index()
            for cls, count in true_dist.items():
                percentage = count / len(self.y_true) * 100
                f.write(f"  ç±»åˆ« {cls}: {count} ({percentage:.2f}%)\n")
            
            f.write("\né¢„æµ‹ç»“æœåˆ†å¸ƒ:\n")
            pred_dist = self.y_pred.value_counts().sort_index()
            for cls, count in pred_dist.items():
                percentage = count / len(self.y_pred) * 100
                f.write(f"  ç±»åˆ« {cls}: {count} ({percentage:.2f}%)\n")
            
            f.write("\n")
            
            # 2. æ€»ä½“æŒ‡æ ‡
            f.write("=" * 80 + "\n")
            f.write("2. æ€»ä½“æŒ‡æ ‡\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"å‡†ç¡®ç‡ (Accuracy):           {self.metrics['accuracy']:.4f} ({self.metrics['accuracy']*100:.2f}%)\n")
            f.write(f"Cohen's Kappa:               {self.metrics['kappa']:.4f}\n")
            f.write(f"Matthewsç›¸å…³ç³»æ•° (MCC):      {self.metrics['mcc']:.4f}\n")
            f.write(f"HammingæŸå¤±:                 {self.metrics['hamming_loss']:.4f}\n")
            f.write("\n")
            
            # 3. å®å¹³å‡æŒ‡æ ‡
            f.write("=" * 80 + "\n")
            f.write("3. å®å¹³å‡æŒ‡æ ‡ (Macro Average)\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ç²¾ç¡®ç‡ (Precision):          {self.metrics['precision_macro']:.4f}\n")
            f.write(f"å¬å›ç‡ (Recall):             {self.metrics['recall_macro']:.4f}\n")
            f.write(f"F1åˆ†æ•° (F1-Score):           {self.metrics['f1_macro']:.4f}\n")
            f.write(f"Jaccardåˆ†æ•°:                 {self.metrics['jaccard_macro']:.4f}\n")
            f.write("\n")
            
            # 4. åŠ æƒå¹³å‡æŒ‡æ ‡
            f.write("=" * 80 + "\n")
            f.write("4. åŠ æƒå¹³å‡æŒ‡æ ‡ (Weighted Average)\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ç²¾ç¡®ç‡ (Precision):          {self.metrics['precision_weighted']:.4f}\n")
            f.write(f"å¬å›ç‡ (Recall):             {self.metrics['recall_weighted']:.4f}\n")
            f.write(f"F1åˆ†æ•° (F1-Score):           {self.metrics['f1_weighted']:.4f}\n")
            f.write(f"Jaccardåˆ†æ•°:                 {self.metrics['jaccard_weighted']:.4f}\n")
            f.write("\n")
            
            # 5. å¾®å¹³å‡æŒ‡æ ‡
            f.write("=" * 80 + "\n")
            f.write("5. å¾®å¹³å‡æŒ‡æ ‡ (Micro Average)\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ç²¾ç¡®ç‡ (Precision):          {self.metrics['precision_micro']:.4f}\n")
            f.write(f"å¬å›ç‡ (Recall):             {self.metrics['recall_micro']:.4f}\n")
            f.write(f"F1åˆ†æ•° (F1-Score):           {self.metrics['f1_micro']:.4f}\n")
            f.write("\n")
            
            # 6. æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
            f.write("=" * 80 + "\n")
            f.write("6. æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡\n")
            f.write("=" * 80 + "\n\n")
            
            class_names = {0: 'å…¶ä»–åœºæ™¯', 1: 'è¿›å…¥åœ°åº“', 2: 'å‡ºåœ°åº“'}
            
            f.write(f"{'ç±»åˆ«':<10} {'ç±»åˆ«å':<12} {'ç²¾ç¡®ç‡':<12} {'å¬å›ç‡':<12} "
                   f"{'F1åˆ†æ•°':<12} {'æ”¯æŒæ•°':<10} {'é”™è¯¯ç‡':<12} {'Jaccard':<12}\n")
            f.write("-" * 80 + "\n")
            
            for cls in self.metrics['classes']:
                cls_name = class_names.get(cls, f'ç±»åˆ«{cls}')
                precision = self.metrics['precision_per_class'][cls]
                recall = self.metrics['recall_per_class'][cls]
                f1 = self.metrics['f1_per_class'][cls]
                support = self.metrics['support'][cls]
                error_rate = self.metrics['error_rate_per_class'][cls]
                jaccard = self.metrics['jaccard_per_class'][cls]
                
                f.write(f"{cls:<10} {cls_name:<12} {precision:<12.4f} {recall:<12.4f} "
                       f"{f1:<12.4f} {support:<10} {error_rate:<12.4f} {jaccard:<12.4f}\n")
            
            f.write("\n")
            
            # 7. æ··æ·†çŸ©é˜µ
            f.write("=" * 80 + "\n")
            f.write("7. æ··æ·†çŸ©é˜µ (Confusion Matrix)\n")
            f.write("=" * 80 + "\n\n")
            
            cm = self.metrics['confusion_matrix']
            classes = self.metrics['classes']
            
            # è¡¨å¤´
            header = 'çœŸå®\\é¢„æµ‹'
            f.write(f"{header:<12}")
            for cls in classes:
                cls_name = class_names.get(cls, f'ç±»åˆ«{cls}')
                f.write(f"{cls_name:<12}")
            f.write("æ€»è®¡\n")
            f.write("-" * 80 + "\n")
            
            # çŸ©é˜µå†…å®¹
            for i, true_cls in enumerate(classes):
                true_name = class_names.get(true_cls, f'ç±»åˆ«{true_cls}')
                f.write(f"{true_name:<12}")
                row_sum = 0
                for j, pred_cls in enumerate(classes):
                    value = cm[i, j]
                    row_sum += value
                    f.write(f"{value:<12}")
                f.write(f"{row_sum}\n")
            
            # åˆ—æ€»è®¡
            f.write("-" * 80 + "\n")
            f.write(f"{'æ€»è®¡':<12}")
            col_sums = cm.sum(axis=0)
            for j, pred_cls in enumerate(classes):
                f.write(f"{col_sums[j]:<12}")
            f.write(f"{cm.sum()}\n")
            f.write("\n")
            
            # æ··æ·†çŸ©é˜µç™¾åˆ†æ¯”
            f.write("æ··æ·†çŸ©é˜µ (ç™¾åˆ†æ¯”):\n")
            header = 'çœŸå®\\é¢„æµ‹'
            f.write(f"{header:<12}")
            for cls in classes:
                cls_name = class_names.get(cls, f'ç±»åˆ«{cls}')
                f.write(f"{cls_name:<12}")
            f.write("\n")
            f.write("-" * 80 + "\n")
            
            for i, true_cls in enumerate(classes):
                true_name = class_names.get(true_cls, f'ç±»åˆ«{true_cls}')
                f.write(f"{true_name:<12}")
                row_sum = cm[i, :].sum()
                if row_sum > 0:
                    for j, pred_cls in enumerate(classes):
                        value = cm[i, j]
                        percentage = value / row_sum * 100
                        f.write(f"{percentage:>6.2f}%     ")
                else:
                    for j in range(len(classes)):
                        f.write(f"{'0.00%':<12}")
                f.write("\n")
            
            f.write("\n")
            
            # 8. åˆ†ç±»æŠ¥å‘Šï¼ˆsklearnæ ¼å¼ï¼‰
            f.write("=" * 80 + "\n")
            f.write("8. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (Classification Report)\n")
            f.write("=" * 80 + "\n\n")
            
            report = self.metrics['classification_report']
            
            # æ¯ä¸ªç±»åˆ«çš„æŠ¥å‘Š
            f.write(f"{'ç±»åˆ«':<12} {'ç²¾ç¡®ç‡':<12} {'å¬å›ç‡':<12} {'F1åˆ†æ•°':<12} {'æ”¯æŒæ•°':<10}\n")
            f.write("-" * 60 + "\n")
            
            for cls in classes:
                cls_name = class_names.get(cls, f'ç±»åˆ«{cls}')
                if str(cls) in report:
                    precision = report[str(cls)]['precision']
                    recall = report[str(cls)]['recall']
                    f1 = report[str(cls)]['f1-score']
                    support = report[str(cls)]['support']
                    f.write(f"{cls_name:<12} {precision:<12.4f} {recall:<12.4f} "
                           f"{f1:<12.4f} {support:<10}\n")
            
            f.write("-" * 60 + "\n")
            f.write(f"{'å®å¹³å‡':<12} {report['macro avg']['precision']:<12.4f} "
                   f"{report['macro avg']['recall']:<12.4f} "
                   f"{report['macro avg']['f1-score']:<12.4f} "
                   f"{report['macro avg']['support']:<10}\n")
            f.write(f"{'åŠ æƒå¹³å‡':<12} {report['weighted avg']['precision']:<12.4f} "
                   f"{report['weighted avg']['recall']:<12.4f} "
                   f"{report['weighted avg']['f1-score']:<12.4f} "
                   f"{report['weighted avg']['support']:<10}\n")
            f.write("\n")
            
            # 9. é”™è¯¯åˆ†æ
            f.write("=" * 80 + "\n")
            f.write("9. é”™è¯¯åˆ†æ\n")
            f.write("=" * 80 + "\n\n")
            
            total_errors = np.sum(self.y_true != self.y_pred)
            f.write(f"æ€»é”™è¯¯æ•°: {total_errors} / {len(self.y_true)} "
                   f"({total_errors/len(self.y_true)*100:.2f}%)\n\n")
            
            # æ¯ä¸ªç±»åˆ«çš„é”™è¯¯åˆ†æ
            f.write("æ¯ä¸ªç±»åˆ«çš„é”™è¯¯æƒ…å†µ:\n")
            for cls in classes:
                cls_name = class_names.get(cls, f'ç±»åˆ«{cls}')
                true_mask = self.y_true == cls
                pred_mask = self.y_pred == cls
                
                correct = np.sum(true_mask & pred_mask)
                total = np.sum(true_mask)
                errors = total - correct
                
                if total > 0:
                    error_rate = errors / total * 100
                    f.write(f"  {cls_name} (ç±»åˆ«{cls}): {errors}/{total} é”™è¯¯ "
                           f"({error_rate:.2f}%), æ­£ç¡®: {correct}/{total} "
                           f"({100-error_rate:.2f}%)\n")
            
            f.write("\n")
            
            # 10. æ··æ·†çŸ©é˜µè¯¦ç»†åˆ†æ
            f.write("=" * 80 + "\n")
            f.write("10. æ··æ·†çŸ©é˜µè¯¦ç»†åˆ†æ\n")
            f.write("=" * 80 + "\n\n")
            
            for i, true_cls in enumerate(classes):
                true_name = class_names.get(true_cls, f'ç±»åˆ«{true_cls}')
                f.write(f"çœŸå®ç±»åˆ«: {true_name} (ç±»åˆ«{true_cls})\n")
                row_sum = cm[i, :].sum()
                if row_sum > 0:
                    for j, pred_cls in enumerate(classes):
                        pred_name = class_names.get(pred_cls, f'ç±»åˆ«{pred_cls}')
                        value = cm[i, j]
                        percentage = value / row_sum * 100 if row_sum > 0 else 0
                        if i == j:
                            f.write(f"  -> æ­£ç¡®é¢„æµ‹ä¸º {pred_name}: {value} ({percentage:.2f}%)\n")
                        else:
                            f.write(f"  -> é”™è¯¯é¢„æµ‹ä¸º {pred_name}: {value} ({percentage:.2f}%)\n")
                f.write("\n")
            
            # æŠ¥å‘Šå°¾éƒ¨
            f.write("=" * 80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
            f.write("=" * 80 + "\n")
        
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def generate_evaluation_report(self, model_dir='model/', output_path=None):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆç±»ä¼¼randomæ–¹æ¡ˆçš„æ ¼å¼ï¼ŒåŒ…å«å‡†ç¡®ç‡ã€æ—¶å»¶ã€è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¤§å°ï¼‰
        
        Parameters:
        -----------
        model_dir : str
            æ¨¡å‹ç›®å½•
        output_path : str
            è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        print("\n" + "=" * 60)
        print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆåŒ…å«å‡†ç¡®ç‡ã€æ—¶å»¶ã€è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¤§å°ï¼‰")
        print("=" * 60)
        
        # 1. è®¡ç®—å‡†ç¡®ç‡ï¼ˆä½¿ç”¨å·²æœ‰çš„æ•°æ®ï¼‰
        if self.y_true is None or self.y_pred is None:
            self.load_data()
        
        accuracy = accuracy_score(self.y_true, self.y_pred)
        
        # 2. åŠ è½½æ¨¡å‹å’Œé¢„æµ‹å™¨æ¥è®¡ç®—æ—¶å»¶ã€æ•ˆç‡ã€æ¨¡å‹å¤§å°
        latency_stats = None
        samples_per_second = None
        avg_time_per_sample_ms = None
        model_info = None
        
        try:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨...")
            predictor = ModelPredictor(
                model_dir=model_dir,
                test_dir='test/',
                output_dir='results/'
            )
            predictor.load_model('improved_lda_model.pkl')
            print("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # å‡†å¤‡ç‰¹å¾æ•°æ®ç”¨äºæ—¶å»¶å’Œæ•ˆç‡æµ‹è¯•
            print("æ­£åœ¨å‡†å¤‡ç‰¹å¾æ•°æ®...")
            exclude_cols = ['time', 'group_name', 'sufficient_window_size', 
                           'labelMovement', 'result', 'labelArea']
            feature_cols = [col for col in self.df.columns if col not in exclude_cols]
            
            if predictor.cleaner.feature_columns is not None:
                feature_cols = [col for col in predictor.cleaner.feature_columns if col in feature_cols]
                X_sample_df = self.df[feature_cols].iloc[0:1].copy()
                missing_cols = set(predictor.cleaner.feature_columns) - set(feature_cols)
                if missing_cols:
                    for col in missing_cols:
                        X_sample_df[col] = 0
                X_sample_df = X_sample_df[predictor.cleaner.feature_columns]
                X_sample = X_sample_df.values
            else:
                X_sample = self.df[feature_cols].iloc[0:1].values
            
            # è®¡ç®—æ£€æµ‹æ—¶å»¶ï¼ˆä¼ å…¥åŸå§‹ç‰¹å¾ï¼Œè®©predict_single_sampleå†…éƒ¨å¤„ç†æ ‡å‡†åŒ–ã€SVDã€QRï¼‰
            print("æ­£åœ¨è®¡ç®—æ£€æµ‹æ—¶å»¶...")
            latency_stats = predictor.calculate_detection_latency(X_sample[0], n_iterations=1000)
            
            # è®¡ç®—æ•ˆç‡ï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼Œä¼ å…¥åŸå§‹ç‰¹å¾ï¼Œè®©predict_batchå†…éƒ¨å¤„ç†æ ‡å‡†åŒ–ã€SVDã€QRï¼‰
            print("æ­£åœ¨è®¡ç®—è®¡ç®—æ•ˆç‡...")
            if predictor.cleaner.feature_columns is not None:
                X_batch_df = self.df[[col for col in predictor.cleaner.feature_columns if col in self.df.columns]].copy()
                missing_cols = set(predictor.cleaner.feature_columns) - set(X_batch_df.columns)
                if missing_cols:
                    for col in missing_cols:
                        X_batch_df[col] = 0
                X_batch_df = X_batch_df[predictor.cleaner.feature_columns]
                X_batch = X_batch_df.values
            else:
                X_batch = self.df[feature_cols].values
            
            # ç›´æ¥ä¼ å…¥åŸå§‹ç‰¹å¾ï¼Œpredict_batchä¼šå†…éƒ¨å¤„ç†æ ‡å‡†åŒ–ã€SVDã€QR
            samples_per_second = predictor.calculate_computational_efficiency(X_batch)
            avg_time_per_sample_ms = 1000 / samples_per_second if samples_per_second > 0 else 0
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            print("æ­£åœ¨è®¡ç®—æ¨¡å‹å¤§å°...")
            model_info = predictor.calculate_model_size()
            
            print("æ‰€æœ‰æŒ‡æ ‡è®¡ç®—å®Œæˆ")
            
        except FileNotFoundError as e:
            print(f"\né”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            print("è¯·ç¡®ä¿å·²è¿è¡Œ b_æ¨¡å‹è®­ç»ƒä»£ç .py è®­ç»ƒæ¨¡å‹")
            print(f"æ¨¡å‹æ–‡ä»¶è·¯å¾„åº”ä¸º: {os.path.join(model_dir, 'improved_lda_model.pkl')}")
            print(f"é¢„å¤„ç†å™¨æ–‡ä»¶è·¯å¾„åº”ä¸º: {os.path.join(model_dir, 'preprocessor.pkl')}")
        except Exception as e:
            import traceback
            print(f"\né”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹è®¡ç®—æ—¶å»¶å’Œæ•ˆç‡")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            print("\nå°†åªç”Ÿæˆå‡†ç¡®ç‡æŠ¥å‘Š")
            print("\næç¤º: è¯·ç¡®ä¿:")
            print("  1. å·²è¿è¡Œ b_æ¨¡å‹è®­ç»ƒä»£ç .py è®­ç»ƒæ¨¡å‹")
            print("  2. æ¨¡å‹æ–‡ä»¶ model/improved_lda_model.pkl å­˜åœ¨")
            print("  3. é¢„å¤„ç†å™¨æ–‡ä»¶ model/preprocessor.pkl å­˜åœ¨")
        
        # ç”ŸæˆæŠ¥å‘Š
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = os.path.basename(self.csv_path).replace('.csv', '')
            output_path = os.path.join(self.output_dir, f'evaluation_report_{timestamp}.txt')
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write("\n")
            
            f.write("[1] é¢„æµ‹å‡†ç¡®ç‡\n")
            f.write(f"å‡†ç¡®ç‡: {accuracy:.4f}\n")
            f.write("\n")
            
            if latency_stats is not None:
                f.write("[2] æ£€æµ‹æ—¶å»¶\n")
                f.write(f"å‡å€¼: {latency_stats['mean']:.4f} ms\n")
                f.write(f"ä¸­ä½æ•°: {latency_stats['median']:.4f} ms\n")
                f.write(f"P95: {latency_stats['p95']:.4f} ms\n")
                f.write(f"P99: {latency_stats['p99']:.4f} ms\n")
                f.write("\n")
            else:
                f.write("[2] æ£€æµ‹æ—¶å»¶\n")
                f.write("æ— æ³•è®¡ç®—ï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰\n")
                f.write("\n")
            
            if samples_per_second is not None:
                f.write("[3] è®¡ç®—æ•ˆç‡\n")
                f.write(f"ååé‡: {samples_per_second:.2f} æ ·æœ¬/ç§’\n")
                f.write(f"å¹³å‡å•æ ·æœ¬è€—æ—¶: {avg_time_per_sample_ms:.4f} ms\n")
                f.write("\n")
            else:
                f.write("[3] è®¡ç®—æ•ˆç‡\n")
                f.write("æ— æ³•è®¡ç®—ï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰\n")
                f.write("\n")
            
            if model_info is not None:
                f.write("[4] æ¨¡å‹å¤§å°\n")
                f.write(f"æ–‡ä»¶å¤§å°: {model_info['file_size_mb']:.2f} MB\n")
                f.write(f"SVDé™ç»´ç»´åº¦: {model_info['n_params_svd']}\n")
                f.write(f"QRç­›é€‰ç‰¹å¾æ•°: {model_info['n_params_qr']}\n")
                if 'n_components_lda' in model_info:
                    f.write(f"LDAåˆ¤åˆ«å‘é‡æ•°é‡: {model_info['n_components_lda']}\n")
                f.write(f"LDAåˆ¤åˆ«å‘é‡æ€»ç»´åº¦: {model_info['n_params_lda']}\n")
                f.write(f"æ€»å‚æ•°é‡: {model_info['total_params']}\n")
            else:
                f.write("[4] æ¨¡å‹å¤§å°\n")
                f.write("æ— æ³•è®¡ç®—ï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰\n")
        
        print(f"\nè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return output_path


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæŒ‡æ ‡è®¡ç®—å’ŒæŠ¥å‘Šç”Ÿæˆ"""
    print("=" * 60)
    print("æŒ‡æ ‡è®¡ç®—å’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("=" * 60)
    
    # ç¤ºä¾‹ï¼šè®¡ç®—æ¸…æ´—è®­ç»ƒæ•°æ®çš„æŒ‡æ ‡
    csv_path = 'train/æ¸…æ´—æµ‹è¯•æ•°æ®.csv'
    
    if os.path.exists(csv_path):
        calculator = MetricsCalculator(
            csv_path=csv_path,
            label_column='labelArea',
            result_column='result',
            output_dir='results/'
        )
        
        # åŠ è½½æ•°æ®
        calculator.load_data()
        
        # è®¡ç®—æŒ‡æ ‡
        calculator.calculate_all_metrics()
        
        # ç”Ÿæˆè¯¦ç»†æŒ‡æ ‡æŠ¥å‘Šï¼ˆåŒ…å«æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šç­‰ï¼‰
        calculator.generate_report()
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆåŒ…å«å‡†ç¡®ç‡ã€æ—¶å»¶ã€è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¤§å°ï¼‰
        calculator.generate_evaluation_report(
            model_dir='model/',
            output_path='results/evaluation_report.txt'
        )
        
        print("\n" + "=" * 60)
        print("æŒ‡æ ‡è®¡ç®—å’ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print("=" * 60)
        print("\nç”Ÿæˆçš„æ–‡ä»¶ï¼š")
        print("  1. è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Šï¼ˆåŒ…å«æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šç­‰ï¼‰")
        print("  2. è¯„ä¼°æŠ¥å‘Šï¼ˆevaluation_report.txtï¼ŒåŒ…å«å‡†ç¡®ç‡ã€æ—¶å»¶ã€è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¤§å°ï¼‰")
    else:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        print("\nå¯ä»¥ä¿®æ”¹mainå‡½æ•°ä¸­çš„csv_pathæ¥æŒ‡å®šè¦åˆ†æçš„æ–‡ä»¶")


if __name__ == '__main__':
    main()

